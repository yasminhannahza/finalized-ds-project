{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f228295",
   "metadata": {},
   "source": [
    "# 1. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54d8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e45bb8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no': 0, 'please': 1, 'thanks': 2, 'welcome': 3, 'yes': 4, 'noaction': 5}\n",
      "X Shape: (360, 30, 258)\n",
      "y Shape: (360, 6)\n",
      "X_train Shape: (288, 30, 258)\n",
      "y_train Shape: (288, 6)\n",
      "X_test Shape: (72, 30, 258)\n",
      "y_test Shape: (72, 6)\n"
     ]
    }
   ],
   "source": [
    "#Define Path to training data of numpy arrays\n",
    "#DATA_PATH = os.path.join('MP_Data_bp_hp')\n",
    "#DATA_PATH = os.path.join('MP_Data-6classes') # including facemesh\n",
    "\n",
    "DATA_PATH = os.path.join('MP_Data') # including facemesh\n",
    "\n",
    "# Define Model Run\n",
    "run = 'run'\n",
    "\n",
    "#Define directory to save training graphs and confusion matrices\n",
    "img_dir = f'Logs/{run}/images'\n",
    "\n",
    "# create directory if image directory does not exist\n",
    "if not os.path.exists(img_dir):\n",
    "    os.makedirs(img_dir)    \n",
    "\n",
    "# Define actions that we try to detect\n",
    "actions = np.array(['no', 'please', 'thanks', 'welcome', 'yes', 'noaction'])\n",
    "\n",
    "# Sixty videos worth of data for each action\n",
    "no_sequences = 60\n",
    "# Videos are going to be 30 frames in length (30 frames of data for each action)\n",
    "sequence_length = 30\n",
    "\n",
    "#create label map dictionary\n",
    "label_map = {label:num for num, label in enumerate(actions)} \n",
    "print(label_map)\n",
    "\n",
    "#sequences represent x data, labels represent y data/the action classes.\n",
    "sequences, labels = [], []\n",
    "#Loop through the action classes you want to detect\n",
    "for action in actions:\n",
    "    #loop through each sequence\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "# X = Training Data that contains spatial coordinates x,y,z of landmarks\n",
    "X = np.array(sequences)\n",
    "\n",
    "# y = categorical labels\n",
    "y = to_categorical(labels).astype(int) #one-hot-encoding to catergorical variable\n",
    "\n",
    "print('X Shape:',X.shape)\n",
    "print('y Shape:',y.shape)\n",
    "\n",
    "# train test split (80% train,20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,stratify=y)\n",
    "\n",
    "print('X_train Shape:',X_train.shape)\n",
    "print('y_train Shape:',y_train.shape)\n",
    "print('X_test Shape:',X_test.shape)\n",
    "print('y_test Shape:',y_test.shape)\n",
    "\n",
    "# split imbalanced dataset into train and test sets with stratification\n",
    "test_count_label = tf.reduce_sum(y_test, axis=0)\n",
    "train_count_label = tf.reduce_sum(y_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "948fd40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no', 'please', 'thanks', 'welcome', 'yes', 'noaction']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show categorical list\n",
    "actions.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2acb89eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_names</th>\n",
       "      <th>train_count</th>\n",
       "      <th>test_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>please</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thanks</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>welcome</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>noaction</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class_names  train_count  test_count\n",
       "0          no           48          12\n",
       "1      please           48          12\n",
       "2      thanks           48          12\n",
       "3     welcome           48          12\n",
       "4         yes           48          12\n",
       "5    noaction           48          12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left = pd.DataFrame(train_count_label,columns=['train_count'])\n",
    "right = pd.DataFrame(test_count_label,columns=['test_count'])\n",
    "df = left.join(right)\n",
    "\n",
    "actions_list = actions.tolist()\n",
    "\n",
    "left = pd.DataFrame(actions_list,columns=['class_names'])\n",
    "df = left.join(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f06b21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_names</th>\n",
       "      <th>train_count</th>\n",
       "      <th>test_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>please</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thanks</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>welcome</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>noaction</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class_names  train_count  test_count\n",
       "0          no           48          12\n",
       "1      please           48          12\n",
       "2      thanks           48          12\n",
       "3     welcome           48          12\n",
       "4         yes           48          12\n",
       "5    noaction           48          12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left = pd.DataFrame(train_count_label,columns=['train_count'])\n",
    "right = pd.DataFrame(test_count_label,columns=['test_count'])\n",
    "df = left.join(right)\n",
    "\n",
    "actions_list = actions.tolist()\n",
    "\n",
    "left = pd.DataFrame(actions_list,columns=['class_names'])\n",
    "df = left.join(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb64bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Distribution of Train & Test Data after train_test_split\n",
    "ax = df.plot.bar(x='class_names',rot=0)\n",
    "ax.set_title('Distribution of Train & Test Data after train_test_split')\n",
    "ax.figure.savefig('Logs/run/images/train_test_distribution.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51f05ab",
   "metadata": {},
   "source": [
    "# 2. Training LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76c2b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard,EarlyStopping\n",
    "\n",
    "#Set up log directory to monitor training accuracy while training\n",
    "log_dir = os.path.join('Logs/{}'.format(run))\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model_dir = 'Logs/{}/model_LSTM'.format(run)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ab991a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM Model Architecture Layers using Keras high-level\n",
    "\n",
    "modelLSTM = Sequential()\n",
    "modelLSTM.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,258))) #each video has input shape of 30 frames of 1662 keypoints: X.shape\n",
    "modelLSTM.add(Dropout(0.2))\n",
    "modelLSTM.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "modelLSTM.add(Dropout(0.2))\n",
    "modelLSTM.add(LSTM(64, return_sequences=False, activation='relu')) \n",
    "modelLSTM.add(Dense(32, activation='relu'))\n",
    "modelLSTM.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d965da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile defines the loss function, the optimizer and the metrics. \n",
    "modelLSTM.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2692e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "#checkpoint_dir = f\"Logs/{run}/tmp/checkpoint\"\n",
    "model_filename = \"Epoch-{epoch:02d}-Loss-{val_loss:.2f}.h5\"\n",
    "checkpoint_filepath = os.path.join('model_LSTM/',model_filename)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=f'Logs/{run}/'+checkpoint_filepath,\n",
    "    monitor='val_loss', #get the minimum validation loss\n",
    "    mode='min',\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    verbose=2)\n",
    "\n",
    "# Reference: https://towardsdatascience.com/a-practical-introduction-to-early-stopping-in-machine-learning-550ac88bc8fd\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=20,mode='auto',verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN IF DONT WANT TO TRAIN\n",
    "# Reference: https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "print(f\"Fit model on training data for {epochs} epochs\")\n",
    "history_LSTM = modelLSTM.fit(\n",
    "    X_train, y_train, \n",
    "    #batch_size=64,\n",
    "    epochs=epochs,\n",
    "    # We pass some validation data for monitoring validation loss and metrics at the end of each epoch\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=2,\n",
    "    batch_size=32,\n",
    "    callbacks=[tb_callback, model_checkpoint_callback, early_stopping]\n",
    ")\n",
    "\n",
    "#https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea955a",
   "metadata": {},
   "source": [
    "# 3. Plot Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9637d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history_LSTM.history) \n",
    "\n",
    "hist_df.to_csv(f'./Logs/{run}/history.csv',index = False)\n",
    "\n",
    "df_hist = pd.read_csv(f'./Logs/{run}/history.csv')\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss_epoch = hist_df[hist_df['loss']==min(hist_df['loss'])].index.values\n",
    "min_loss = min(hist_df['loss'])\n",
    "print('Index of Minimum Loss =',min_loss_epoch[0])\n",
    "print('Minimum Loss =',round(min_loss,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e724b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history_LSTM.history['categorical_accuracy'])\n",
    "plt.plot(history_LSTM.history['val_categorical_accuracy'])\n",
    "plt.title('Model Training and Validation Categorical Accuracy LSTM')\n",
    "plt.ylabel('categorical_accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.savefig(f'{img_dir}/Model Training and Validation Categorical Accuracy LSTM.jpg')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history_LSTM.history['loss'])\n",
    "plt.plot(history_LSTM.history['val_loss'])\n",
    "plt.title('Model Training and Validation Loss LSTM')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.savefig(f'{img_dir}/Model Training and Validation Loss LSTM.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0540877a",
   "metadata": {},
   "source": [
    "# 4. Load Best Model Weights/Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e578d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH: \"C:\\Users\\yasmi\\Desktop\\data science\\Logs\\run_version2training\\model\\Epoch-72-Loss-0.15.h5\"\n",
    "\n",
    "modelLSTM.load_weights(\"C:/Users/yasmi/Desktop/finalized ds project/Logs/run/model_LSTM/Epoch-11-Loss-0.08.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6275ed2",
   "metadata": {},
   "source": [
    "# 7. Make Predictions on X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f590f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = modelLSTM.predict(X_test)\n",
    "\n",
    "# Get y_predict and apply softmax function\n",
    "np.argmax(res[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f596123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted Action\n",
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba47e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Action\n",
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af712a97",
   "metadata": {},
   "source": [
    "# 8. Model Evaluation (Categorical Accuracy and Confusion Matrix)\n",
    "Running these cells converts the predicition from their one-hot encoded representation to a categorical label e.g. 0,1 or 2 as opoosed to [1,0,0], [0,1,0] or [0,0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0775c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bfd66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['no', 'please', 'thanks', 'welcome', 'yes', 'noaction']\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f99c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = list(label_map.keys())\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bed208",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confusion_matrix_accuracy(X_test,y_test,'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc1585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# predict on test data\n",
    "y_pred = modelLSTM.predict(X_test)\n",
    "\n",
    "# convert predictions to binary labels\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# calculate precision, recall, f1-score, and accuracy\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1-Score: \", f1_score)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c2c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,221,173), (245,185,265), (146,235,193),(204,152,295),(255,217,179),(0,0,179)]\n",
    "\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "\n",
    "#collect 30 frames\n",
    "sequence = []\n",
    "\n",
    "#concantenate \n",
    "sentence = []\n",
    "\n",
    "#confident matrix\n",
    "threshold = 0.8\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "#         sequence.insert(0,keypoints)\n",
    "#         sequence = sequence[:30]\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
